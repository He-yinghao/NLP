# NLP下游任务

1. **文本分类**：
   - **情感分析**：识别文本中的情感倾向（正面、负面或中性）。
   - **主题分类**：将文档归类到预定义的主题类别中。
   - **垃圾邮件检测**：识别并过滤垃圾邮件。
   - **意图识别**：在对话系统中识别用户的意图。
2. **命名实体识别（NER）**：
   - 识别文本中的实体（如人名、地名、组织名等）并进行分类。
3. **关系抽取**：
   - 从文本中提取实体之间的关系，例如“人-组织”、“地点-事件”等。
4. **问答系统（QA）**：
   - **阅读理解**：基于给定的上下文回答问题。
   - **开放域问答**：不依赖于具体文档，直接从知识库或互联网中检索答案。
5. **机器翻译**：
   - 将一种自然语言自动翻译成另一种自然语言。
6. **文本生成**：
   - **摘要生成**：自动生成文章或文档的摘要。
   - **对话生成**：生成自然流畅的对话回复。
   - **故事生成**：根据给定的提示生成连贯的故事。
   - **代码生成**：根据自然语言描述生成编程代码。
7. **信息抽取**：
   - 从非结构化文本中提取结构化信息，如事件抽取、时间抽取等。
8. **语义角色标注（SRL）**：
   - 标注句子中谓词及其论元的角色关系。
9. **指代消解**：
   - 解决文本中的代词或其他指示词所指的具体对象。
10. **多模态任务**：
    - **图像字幕生成**：为图片生成描述性的文字。
    - **视觉问答**：结合图像和文本信息回答问题。
    - **跨模态检索**：根据一种模态的信息（如文本）检索另一种模态的信息（如图像）。
11. **文本相似度计算**：
    - **句子对分类**：判断两个句子是否具有某种关系（如蕴含、矛盾或中立）。
    - **文本匹配**：用于搜索、推荐系统等场景中的文本相似度计算。
12. **语法分析**：
    - **依存句法分析**：解析句子中词与词之间的依存关系。
    - **成分句法分析**：解析句子的层次结构。
13. **对话系统**：
    - **任务型对话**：完成特定任务（如订餐、预订机票）的对话系统。
    - **闲聊机器人**：与用户进行自然对话的聊天机器人。

# NLP任务处理步骤

## 数据预处理

### 分词

**将句子拆解为词或子词的过程**

- **基于空格的分词**：在英语等以空格分隔的语言中，简单分词即可，但对像中文这样没有空格的语言效果不好。
- **BPE（Byte-Pair Encoding）**：BPE可以将词分解成子词单元，使模型能够处理未知词汇并减少词表大小。可以使用工具 [sentencepiece](https://github.com/google/sentencepiece) 或 [subword-nmt](https://github.com/rsennrich/subword-nmt) 进行BPE分词。
- **WordPiece**：类似于BPE的分词方法，广泛应用于BERT等模型中。

### 转换为小写

- 对于**不区分大小写的模型训练**，一般将文本转换为小写。
- 对于一些语言，**保留大小写**可能有利于性能，如**区分专有名词**等，所以是否转换取决于具体需求。

### 去除标点与特殊字符

- 根据需要去除或替换标点

### 长度过滤

**过长或过短的句子会影响模型训练的稳定性**

可以根据句子长度进行过滤，例如保留长度在 5 到 50 个词之间的句子对，以确保句子有较好的信息量且能被模型合理学习

### 构建词汇表

在分词后，构建词汇表，包括源语言和目标语言。词汇表大小决定了模型的处理能力，过小会丢失信息，过大会增加计算复杂度

**分词的性能对词汇表的构建产生重要影响**

### 将词汇转化为数字

使用词汇表将每个词或子词转换成唯一的ID

常见方法是利用`<UNK>`符号表示**词汇表外**的词，并添加 `<SOS>`、`<EOS>` 标记表示句子开头和结尾

### 数据对齐与填充

因为不同句子长度不一，训练时需要将句子对**填充到相同的长度**（如最长句子的长度），以便进行批处理。一般用`<PAD>`符号来填充

## 词向量

词向量的引入是因为词的 `Token ID` 仅是一个索引，**没有包含词与词之间的任何语义关系**。通过词向量（例如使用 **word2vec** 或 **预训练的嵌入**），可以为词提供一个稠密的向量表示，**将具有相似语义的词分布到相近的位置上**，从而捕捉词的语义信息。词向量可以通过以下方式实现：

- **预训练词向量（如 word2vec、GloVe 等）**：在大量无监督文本数据上训练的词向量，可以**直接用在模型中**。
- **随机初始化的词向量**：在没有大规模预训练的情况下，直接从随机数初始化词向量，**随着模型训练而更新**。

### One-Hot Encoding

### Bag of Words（词袋模型）

### TF-IDF（词频-逆文档频率）

### Word Embedding

#### Word2Vec

##### CBOW（Continuous Bag of Words）

**通过上下文单词预测中心单词**。例如，给定句子 `"the quick brown fox jumps"`，如果上下文窗口大小为 2，那么对于中心词 `"brown"`，其上下文是 `["the", "quick", "fox", "jumps"]`。CBOW 的任务就是利用这些上下文词预测 `"brown"`。

CBOW 的神经网络结构如下：

1. **输入层**：上下文单词的 One-Hot 编码。
2. **隐藏层**：对上下文单词的词向量取平均（或求和）。
3. **输出层**：Softmax 计算每个单词作为中心词的概率。

最终，**输入层到隐藏层的权重矩阵 `W` 就是我们需要的词向量表**。

- **词汇表**：`["the", "quick", "brown", "fox", "jumps"]`（5 个单词）

- **词向量维度 `N` **：3（实际应用通常 100-300 维）

- **上下文窗口大小**：1（即左右各 1 个单词）

- 以 `(context: ["the", "brown"], target: "quick")`为例

- **One-Hot 编码**：

  - `"the"`→ `[1, 0, 0, 0, 0]`
  - `"brown"`→ `[0, 0, 1, 0, 0]`

- **查找输入向量**

  - 初始化权重矩阵 `W (5 * 3)`

    ```python
    W = [
      [0.1, 0.2, 0.3],  # "the"
      [0.4, 0.5, 0.6],  # "quick"
      [0.7, 0.8, 0.9],  # "brown"
      [0.2, 0.3, 0.4],  # "fox"
      [0.5, 0.6, 0.7]   # "jumps"
    ]
    # 单位向量：左行右列
    ```

  - `"the"`的词向量：`[0.1, 0.2, 0.3]`（第一行）
  - `"brown"`的词向量：`[0.7, 0.8, 0.9]`（第三行）

- **计算隐藏层向量**：

  - 对上下文词向量取平均

    ```python
    h = ([0.1, 0.2, 0.3] + [0.7, 0.8, 0.9]) / 2 = [0.4, 0.5, 0.6]
    ```

- **计算输出概率**

  - 假设输出权重矩阵 `T (3 * 5)`

    ```python
    T = [
      [0.1, 0.2, 0.3, 0.4, 0.5],
      [0.6, 0.7, 0.8, 0.9, 1.0],
      [1.1, 1.2, 1.3, 1.4, 1.5]
    ]
    ```

  - 计算得分 `u = h \cdot T`

    ```python
    u = [0.4 * 0.1 + 0.5 * 0.6 + 0.6 * 1.1, ..., ...] = [1.0, 1.16, 1.32, 1.48, 1.64]
    ```

  - Softmax 计算概率

    ```python
    y_hat = softmax([1.0, 1.16, 1.32, 1.48, 1.64]) ≈ [0.15, 0.17, 0.19, 0.21, 0.28]
    ```

  - 我们希望 `"quick"`（第 2 个词）的概率最大，因此计算损失（如交叉熵）并反向传播更新 `W` 和 `T`。

- **训练完成后**：输入权重矩阵 `W` 的每一行就是对应单词的词向量。

$$
L=-\sum_{t=1}^T\sum_{-c\leq j\leq c,j\neq0}\log P(w_{t+i}|w_t)
$$

$$
\begin{align}
T&:是语料库中的总词数\\

w_t&:是当前的中心词\\

c&:是上下文窗口大小，表示在中心词 w_t的左右各考虑多少个词\\

w_{t+j}&:是在当前中心词的上下文中出现的词（词嵌入前的token\ ID）
\end{align}
$$

$$
P(w_{t+j}|w_t)=\frac{exp(v_{w_{t+j}}^Tv_{w_t})}{\sum_{w\in V}exp(v_w^Tv_{w_t})}
$$

$$
\begin{align}
v_{w_{t+j}}&:词w_{t+j}的向量表示\\
v_{w_t}&:词w_t的向量表示\\
V&:词汇表的所有词
\end{align}
$$

##### Skip-Gram

**Skip-Gram**：模型的目标是**给定中心词预测其上下文词**。适用于大数据集，且效果更好。
$$
L=-\sum_{t=1}^T\sum_{-c\leq j\leq c,j\neq0}\log P(w_t|w_{t+i})
$$


序列数据：w_1,w_2,...,w_n （序列长度为n的`token ID`数据）

#### GloVe

#### `FastText`

# 1. n-gram

## 1.1 n-gram定义

`n-gram` 语言模型是自然语言处理（NLP）中的一种基础**统计模型**，用于**预测文本序列中下一个词的概率**。它基于马尔科夫假设，即每个词的出现仅依赖于前面有限个词的出现。

给定单词序列W1,W2,...,WT，语言模型的的目标是计算该序列的概率：
$$
P(w_1,w_2,...,w_T)=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_1,w_2)\cdot ...\cdot P(w_T|w_1,w_2,...,w_{T-1})
$$
直接计算这种联合概率是非常困难的，因为需要考虑非常长的词序列。因此，`n-gram` 语言模型引入了*马尔科夫假设*，**即一个词的出现只与前面固定数量的词有关**，而与更远的词无关。最终简化为：

- `unigram`（1-gram）：每个词出现的概率是独立的
  $$
  P(w_1,w_2,...,w_T)=P(w_1)\cdot P(w_2)\cdot...\cdot P(w_T)
  $$

- `bigram`（2-gram）：每个词只依赖于前一个词
  $$
  P(w_1,w_2,...,w_T)=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_2)\cdot...\cdot P(w_T|w_{T-1})
  $$

- `trigram`（3-gram）：每个词只依赖于前两个词
  $$
  P(w_1,w_2,...,w_T)=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_1,w_2)\cdot...\cdot P(w_T|w_{T-2},w_{T-1})
  $$

- 一般的`n-gram`：每个词只依赖于前n-1个词
  $$
  P(w_1,w_2,...,w_T)=P(w_1)\cdot P(w_2|w_1)\cdot...\cdot P(w_T|w_{T-n+1},w_{T-1})
  $$

`n-gram`**模型的概率计算**
$$
P(w_T|w_{T-n+1},w_{T-1})=\frac{Count(w_{T-n+1},...,w_T)}{Count(w_{T-n+1},...,w_{T-1})}
$$

- $$
  Count(w_{T-n+1},...,w_T):表示词序w_{T-n+1},...,w_T在训练语料中出现的次数
  $$

- $$
  Count(w_{T-n+1},...,w_{T-1}):表示词序w_{T-n+1},...,w_{T-1}在训练语料中出现的次数
  $$

## 1.2 平滑处理

# 2. RNN

**循环神经网络**（Recurrent Neural Network, `RNN`）是一种专门设计用于**处理序列数据**的神经网络。与传统的前馈神经网络不同，RNN可以保持内部状态（或称为记忆），这使得它们能够捕捉时间序列中的依赖关系。

<img srcsrc="..\Images\20200727171017364.png" style="zoom:67%;" >

对当前时间步`t`进行特征提取时，考虑之前时间步的信息（**隐藏状态**）
$$
在每个时间步t，RNN接收一个输入向量x_t，并且结合上一时间步的隐藏状态h_{t-1}来产生当前时间步的隐藏状态h_t。\\输出y_t通常是基于当前隐藏状态h_t计算得到的
$$

- 当前时间步的隐藏状态：当前**时间步的输入**信息与**前一时间步的隐藏状态**（包含了前面序列的所有信息）
  $$
  h_t=f(W_{hh}*h_{t-1}+W_{xh}*x_t+b)
  $$

- 当前时间步输出：当前时间步数据的特征提取
  $$
  y_t=g(W*h_t+b)
  $$

**RNN缺点**：RNN难以学习长时间跨度的信息依赖关系。当序列很长时，梯度可能消失或爆炸，导致训练困难。

# 3. LSTM

LSTM（Long Short-Term Memory）是一种特殊的循环神经网络（RNN），专门用于解决传统RNN在长序列数据中存在的**长期依赖问题**

加入**细胞状态**和**三个门控机制**解决RNN的缺点

<img src="..\Images\d48ee7294f7a4fa7b9525b53250f3622.png" alt="d48ee7294f7a4fa7b9525b53250f3622" style="zoom:50%;" />

1. **遗忘门（Forget Gate）**：决定哪些信息应该从细胞状态中被遗忘。
   $$
   f_t=\sigma(W_f\cdot [h_{t-1},x_t]+b_f)
   $$

2. **输入门（Input Gate）**：决定哪些新的信息应该被添加到细胞状态中。

   - 输入门控制：决定需要更新的信息比例
     $$
     i_t=\sigma (W_i\cdot [h_{t-1},x_t]+b_i)
     $$

   - 候选细胞状态：候选状态使用`tanh`函数生成新的候选信息
     $$
     \tilde{C}_t=tanh(W_C\cdot [h_{t-1},x_t]+b_c)
     $$

3. **细胞状态更新**：结合遗忘门和输入门，更新细胞状态。
   $$
   C_t=f_t*C_{t-1}+i_t*\tilde{C}_t
   $$

4. **输出门（Output Gate）**：决定当前时间步的输出是什么，并影响下一时间步的隐状态。
   $$
   o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)
   $$

5. **隐藏状态**：根据当前细胞状态与输出门计算
   $$
   h_t=o_t*tanh(C_t)
   $$

6. **当前时间步输出**：将当前时间步的隐藏状态放入全连接层中

**LSTM缺点**：

- 结构复杂，包含更多的参数，可能导致更高的内存消耗和计算成本。
- 尽管比RNN有所改进，但在极长的序列上仍然可能遇到梯度消失/爆炸的问题。

# 4. GRU

`GRU`（Gated Recurrent Unit）：解决标准RNN中的梯度消失和梯度爆炸问题，并且相比于另一种流行的RNN变体LSTM（Long Short-Term Memory），GRU拥有**更少的参数**

<img src="..\Images\b7e8bf92262c4fb1aa53c519015cab1b.png" alt="b7e8bf92262c4fb1aa53c519015cab1b" style="zoom: 80%;" />

1. **更新门 (z_t)**：

   - 更新门决定了前一时刻的状态 `h_t-1` 有多少信息会被传递到当前时刻的状态 `h_t` 中
     $$
     z_t = \sigma(W_z[h_{t-1},x_t]+b_z)
     $$

2. **重置门 (r_t)**：

   - 重置门决定了如何将新的输入信息与之前的状态结合
     $$
     r_t=\sigma(W_r[h_{t-1},x_t]+b_r)
     $$

3. **候选隐藏状态 **：

   - 候选隐藏状态是根据当前输入和重置后的先前状态计算出来的
     $$
     \tilde{h}_t=tanh(W_h[r_t*h_{t-1},x_t]+b_h)
     $$

4. **最终隐藏状态 (h_t)**：

   - 最终的隐藏状态是通过**更新门**对**先前状态**和**候选隐藏状态**的**线性组合**来得到的
     $$
     h_t=z_t*h_{t-1}+(1-z_t)*\tilde{h}_t
     $$

**GRU缺点**：

- GRU通常比LSTM更快且参数更少，但在一些复杂的任务中，它可能不如LSTM那样强大
- 与LSTM一样，GRU也存在一定的计算开销，特别是当序列非常长时。

# 5. Transformer

## 5.1 Transformer Model

1. 编码器：将输入序列映射成一个**关注上下文信息的序列**，然后将这个序列映射成 key 和 value 输入到解码器中。

   - **多头自注意力机制 (Multi-Head Self-Attention)**
     - 输入序列的每个词都与序列中其他词计算注意力权重，从而获得全局信息。

   - **前馈神经网络 (Feed-Forward Network, FFN)**
     - 每个位置独立通过一个两层的 MLP，提升表示能力。

   - **残差连接 (Residual) + `LayerNorm`**
     - 保持梯度稳定，避免深层训练困难。

2. 解码器：自回归，将已经预测的值通过 Masked Multi-Head Attention 层再输入到解码器中，将其映射成 query。
   - **Masked Multi-Head Self-Attention**
     - 解码器生成第 t 个词时，只能看到 ≤ t 的词，保证自回归生成。
   - **Encoder-Decoder Attention**
     - 将解码器当前状态与编码器输出进行交互，建立输入与输出的映射关系。
   - **Feed-Forward Network**
     - 和编码器相同。
   - **残差连接 + `LayerNorm`**
     - 稳定训练。

<img src="..\Images\image-20240928154204680.png" alt="image-20240928154204680" style="zoom: 50%;" />

## 5.2 Self-Attention

<img src="..\Images\image-20240928154415608.png" alt="image-20240928154415608" style="zoom:45%;" />

Q表示查询向量，K表示键，V表示值，其中Q和K的维度相同:d_k
$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
## 5.3 Positional Encoding

$$
\begin{align}
PE_{(pos,2i)}&=\sin(\frac{pos}{10000^{2i/d}})\\
PE_{(pos,2i+1)}&=\cos(\frac{pos}{10000^{2i/d}})
\end{align}
$$

- `pos`：位置索引（句子中的单词位置）
- `i`：维度索引（嵌入向量中的维度）
- `d`：嵌入向量维度

正弦和余弦函数使得位置编码具有**周期性**，有助于模型捕捉到长距离依赖关系

## 5.4 Mask

- 填充掩码：序列长度不相同，将短序列进行填充（填充部分不参与注意力计算）

- 因果掩码：防止模型在解码阶段看见未来的单词

## 5.5 Teacher Forcing

在**训练**时：解码器的输入是真实数据的“已预测部分”（即目标序列整体右移一位）。

在**推理（预测）**时：解码器的输入是它自己上一时间步的预测值。

# 6. BERT

